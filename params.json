{"name":"GPU JORProx and SORProx","tagline":"","body":"=\r\nProjective Jacobi and Gauss-Seidel on the GPU for Non-Smooth Multi-Body Systems\r\n=\r\n\r\nThis source code accompanies the paper   \r\n\r\n> **G. NÃ¼tzi et al. , Projective Jacobi and Gauss-Seidel on the GPU for Non-Smooth Multi-Body Systems, 2014** , download : [1](http://proceedings.asmedigitalcollection.asme.org/proceeding.aspx?articleID=2091012) or [2](http://www.zfm.ethz.ch/~nuetzig/_private_files/projective.pdf)\r\n\r\nThe [master thesis](http://dx.doi.org/10.3929/ethz-a-010054012) should be consulted only in the case of being interested in the details of certain GPU variants (see below).\r\n\r\n---------------------------\r\nInstallation & Dependencies\r\n---------------------------\r\nTo build the performance tests (MatrixMultiply, Prox, etc.) you need the built tool [cmake](\r\nhttp://www.cmake.org).\r\nThe performance tests only depend on the matrix library [Eigen](http://eigen.tuxfamily.org) at least version 3. Download it and install it on your system.\r\nYou need CUDA installed on your system as well, download and install the latest [CUDA API and Drivers](https://developer.nvidia.com/cuda-downloads).\r\n\r\nDownload the latest CudaFramework code:\r\n```bash\r\n    $ git clone https://github.com/gabyx/CudaFramework.git CudaFramework  \r\n```\r\nMake a build directory and navigate to it:\r\n```bash\r\n    $ mkdir Build\r\n    $ cd Build\r\n```\r\nInvoke cmake in the Build directory:\r\n```bash\r\n    $ cmake ../CudaFramework\r\n```\r\nThe cmake script will find Eigen and Cuda if you installed it in a system wide folder (e.g ``/usr/local/``)\r\n\r\n\r\nFinally, build the performance tests:\r\n```bash\r\n    $ make all \r\n    $ make install\r\n``` \r\n To build in parallel use the ``-jN`` flag in the `make` commmand, where ``N``denotes the number of parallel threads to use.\r\n\r\n--------------------------\r\nSupported Platforms\r\n--------------------------\r\nThe code has been tested on Linux and OS X with compilers ``clang`` and ``gcc``. \r\nIt should work for Windows as well, but has not been tested.\r\n\r\n\r\n##Example Usage\r\n\r\nThe target ``PerformanceProx`` contains the parallel GPU implementation of the **projective overrelaxed Jacobi (JORProx)** and **succesive overrelaxed Gauss-Seidel (SORProx, SORProxRelaxed)** iterations used in multi-body dynamics.\r\nThe target ``PerformanceMatrix`` contains the performance test of the efficient parallel matrix-multiplication kernel which is used for the JORProx implementation.\r\nThe target ``GaussSeidelTest`` contains the test launches of the parallel linear Gauss-Seidel algorithm.\r\n\r\nThe performance tests are all written in the same structure. \r\nA performance tests of any kind of application can be specified with the ``PerformanceTest`` template class which accepts a test method as template argument.\r\nFor the kernel performance tests mainly used in this project, the test method ``KernelTestMethod`` is of main interest. It is used for profiling/checking a certain GPU implementation (``ProxTestVariant`` see below) against a serial CPU implementation. The results are saved in an XML file, with all output provided by the test method (e.g.  ``KernelTestMethod``). A python notebook``python/ParseXML.pynb`` is provided as an example to parse the performance tests output XML.\r\n\r\n\r\nThe following example shows how a performance test for the SORProx GPU Variant 1 is launched (target: ``PerformaceProx``):\r\n```C++\r\n    typedef KernelTestMethod< /* Specify a kernel test method */ \r\n    \r\n        KernelTestMethodSettings<false,true,0> ,  /* Specify the general kernel test method settings */ \r\n        \r\n        ProxTestVariant< /* Specify the specific variant of the kernel test method */ \r\n            \r\n            ProxSettings< /* All settings for the prox test variant */ \r\n        \r\n               double,                  /* use double floating point precision */ \r\n               SeedGenerator::Fixed<5>, /* the seed for the random value generator for the test data */ \r\n               false,                   /* Write the test data to a file (matlab style) */ \r\n               1,                       /* Max. iterations of the prox iteration */\r\n               false,                   /* Abort iteration if prox iteration converged */\r\n               10,                      /* Convergence check every 10 iterations */\r\n               true,                    /* Compare the GPU implementation to the exact serial replica on th CPU */\r\n               ProxPerformanceTestSettings<3000,20,4000,3>,                /* Problem sizes from 3000 contacts to 4000 in steps of 20, generate 3 random test problems per problem size*/\r\n               SorProxGPUVariantSettings<1,ConvexSets::RPlusAndDisk,true>  /* Use the GPU Variant 1, align the memory on the GPU for coalesced access!*/\r\n            >\r\n            \r\n        >\r\n        \r\n    > test1;\r\n    \r\n    PerformanceTest<test1> A(\"SorProxVariant1D\"); /* output file written to: SorProxVariant1D***.xml*/\r\n    A.run();\r\n```\r\n\r\n### Understanding JORProx and SORProx\r\nTo understand the workflow of the performance tests and application of the **JORProx** and **SORProx**, the user is encouraged to understand the basic workflow of the ``ProxTestVariant`` in ``ProxTestVariant.hpp``.\r\nThis class contains the basic initialization of the used matrices for the numerical iterations, the two important functions\r\n``ProxTestVariant::runOnGPU()`` and ``ProxTestVariant::runOnCPU()`` which run the specified variant (e.g. SORProx GPU Variant 1 in the above example) \r\non the CPU or the GPU, and a check routine\r\n``ProxTestVariant::checkResults()`` which compares the results from the GPU to the CPU.\r\nThe function call ``ProxTestVariant::runOnGPU()`` calls the ``runGPUProfile()`` function of the templated type ``ProxTestVariant::m_gpuVariant``.\r\n\r\n**The GPU variants for the type ``m_gpuVariant`` of the JORProx and SORProx can be found in ``SorProxGPUVariant.hpp`` and ``JorProxGPUVariant.hpp``.** \r\n**These files will help the most in understanding the source code together with the paper.**\r\n\r\nEach GPU variant class ``JorProxGPUVariant`` and ``SorProxGPUVariant`` contains certain variants which correspond to fixed GPU settings (block dimension, threads per block etc...).\r\nThe descriptions of these variants are consistent with the master thesis (and hopefully also the paper).\r\nEach GPU variant has a ``initializeTestProblem()`` function which fills the iteration matrices with random values (keeping the problem size fixed!).\r\nEach GPU variant also has ``runGPUProfile()`` and ``runGPUPlain()`` functions which launch the GPU variants with or without timing information.\r\n\r\nTo get to the bottom of the prox iteration variants, consider the the kernels A and B involved in the GPU variant ``SorProxGPUVariant``. This variant is described in the paper in detail. Kernels A and B are launched sequentially over the iteration matrix ``T_dev`` as shown in the following:\r\n```C\r\n    for(m_nIterGPU=0; m_nIterGPU< m_nMaxIterations ; m_nIterGPU++){\r\n\r\n            // Swap pointers of old and new on the device\r\n            std::swap(x_old_dev.m_pDevice, x_new_dev.m_pDevice);\r\n\r\n            for(int kernelAIdx = 0; kernelAIdx < loops; kernelAIdx++){\r\n\r\n               //cudaThreadSynchronize();\r\n               proxGPU::sorProxContactOrdered_1threads_StepA_kernelWrap<SorProxSettings1>(\r\n                  mu_dev,x_new_dev,T_dev,d_dev,\r\n                  t_dev,\r\n                  kernelAIdx,\r\n                  pConvergedFlag_dev,\r\n                  m_absTOL,m_relTOL);\r\n\r\n               proxGPU::sorProx_StepB_kernelWrap<SorProxSettings1>(\r\n                  t_dev,\r\n                  T_dev,\r\n                  x_new_dev,\r\n                  kernelAIdx\r\n                  );\r\n\r\n            }\r\n    }\r\n```\r\n\r\n### Interfacing with Own Code\r\nThe best way to use the SORProx or JORProx GPU implementations right out of the box is to instantiate the following\r\nvariant types somewhere in your code:\r\n```C++\r\n   JorProxGPUVariant< JorProxGPUVariantSettingsWrapper<PREC,5,ConvexSets::RPlusAndDisk,true,1000,true,10,false, TemplateHelper::Default>, ConvexSets::RPlusAndDisk > m_jorGPUVariant;\r\n\r\n   SorProxGPUVariant< SorProxGPUVariantSettingsWrapper<PREC,1,ConvexSets::RPlusAndDisk,true,1000,true,10,true,  TemplateHelper::Default >,  ConvexSets::RPlusAndDisk > m_sorGPUVariant;\r\n```\r\nPlease see the the file ``ProxSettings.hpp`` for the settings of ``SorProxGPUVariantSettingsWrapper`` and ``JorProxGPUVariantSettingsWrapper``, the number 1000 denotes the maximal number of global prox iterations.\r\nThese variants are the fastest methods so far (at least for the NIVIDIA GTX 580), you can try to tweak the settings in ``ProxKernelSettings.hpp`` for the JORProx and SORProx to gain better speeds for your GPU. \r\n\r\nLaunching the iterations would look similar to this example:\r\n\r\n```C++\r\n        m_jorGPUVariant.setSettings(m_settings.m_MaxIter,m_settings.m_AbsTol,m_settings.m_RelTol);\r\n        gpuSuccess = m_jorGPUVariant.runGPUPlain(P_front,m_T,P_back,m_d,m_mu);\r\n        m_globalIterationCounter = m_jorGPUVariant.m_nIterGPU;\r\n        /* OR */\r\n        m_sorGPUVariant.setSettings(m_settings.m_MaxIter,m_settings.m_AbsTol,m_settings.m_RelTol);\r\n        gpuSuccess = m_sorGPUVariant.runGPUPlain(P_front,m_T,P_back,m_d,m_mu);\r\n        m_globalIterationCounter = m_jorGPUVariant.m_nIterGPU;\r\n```\r\nMatrices ``m_T`` and ``m_d`` are built as described in the paper. Vector ``m_mu`` are the friction coefficients for all contacts which consist of a normal and two tangential forces. The percussions ``P_back`` and ``P_front`` are contact ordered and each contact tuple consits of (normal percussion, tangential percussion 1, tangential percussion 2, see the description in the paper).\r\n\r\n\r\n\r\n--------------------------\r\nLicensing\r\n--------------------------\r\n\r\nThis source code is released under GNU GPL 3.0. \r\n\r\n---------------------------\r\nAuthor and Acknowledgements\r\n---------------------------\r\n\r\nCudaFramework was written by Gabriel NÃ¼tzi. Source code from [ModernGPU](http://www.moderngpu.com) has been used, see ``CudaModern`` folder in ``/include/CudaFramework``\r\n","google":"UA-58742969-1","note":"Don't delete this file! It's used internally to help with page regeneration."}