// ========================================================================================
//  CudaFramework
//  Copyright (C) 2014 by Gabriel NÃ¼tzi <nuetzig (at) imes (d0t) mavt (d0t) ethz (d0t) ch>
//
//  This Source Code Form is subject to the terms of the GNU GPL 3.0 licence.
//  If a copy of the GNU GPL 3.0 was not distributed with this
//  file, you can obtain one at http://opensource.org/licenses/GPL-3.0.
// ========================================================================================

#ifndef CudaFramework_Kernels_ProxGPU_SorProxGPUVariant_hpp
#define CudaFramework_Kernels_ProxGPU_SorProxGPUVariant_hpp

#include <stdlib.h>
#include <stdio.h>
#include <iostream>
#include <fstream>
#include <time.h>
#include <algorithm>

#include <boost/format.hpp>

#include "CudaFramework/General/CPUTimer.hpp"
#include "CudaFramework/General/StaticAssert.hpp"
#include "CudaFramework/General/TypeTraitsHelper.hpp"
#include "CudaFramework/CudaModern/CudaError.hpp"
#include "CudaFramework/General/Utilities.hpp"
#include "CudaFramework/CudaModern/CudaMatrix.hpp"
#include "CudaFramework/CudaModern/CudaMatrixUtilities.hpp"
#include "CudaFramework/General/GPUDefines.hpp"

#include "ConvexSets.hpp"

#include "CudaFramework/General/FlopsCounting.hpp"

#include <Eigen/Dense>

#include <cuda_runtime.h>
#include "cublas_v2.h"

#include "CudaFramework/Kernels/ProxGPU/ProxSettings.hpp"
#include "CudaFramework/Kernels/ProxGPU/ProxGPU.hpp"
#include "CudaFramework/Kernels/ProxGPU/ProxKernelSettings.hpp"

/**
* @addtogroup ProxTestVariant
* @defgroup SorProxGPUVariant Sor Prox GPUVariants
* @detailed VariantId specifies which variant is launched:
* Here the different variants have been included in one class!
* To be more flexible we can also completely reimplement the whole class for another GPUVariant!
* @{
*/

using namespace TypeTraitsHelper;



//Specialisation
template<typename TSorProxGPUVariantSettingsWrapper>
class SorProxGPUVariant<TSorProxGPUVariantSettingsWrapper, ConvexSets::RPlusAndDisk>{
public:

   DEFINE_SorProxGPUVariant_SettingsWrapper(TSorProxGPUVariantSettingsWrapper);

   SorProxGPUVariant()
      :Matlab(Eigen::FullPrecision, 0, ", ", ";\n", "", "", "[", "]"), m_nMaxIterations(nMaxIterations), pConvergedFlag_dev(NULL)
   {

   }
    ~SorProxGPUVariant(){

   }

   void setSettings(unsigned int iterations, PREC absTol, PREC relTol){
      m_nMaxIterations = iterations;
      m_absTOL = absTol;
      m_relTOL = relTol;
   }


   typedef typename ManualOrDefault<  IsEqual<VariantId,1>::result, TKernelSettings,SorProxSettings1RPlusAndDisk>::TValue SorProxSettings1;
   typedef typename ManualOrDefault<  IsEqual<VariantId,2>::result, TKernelSettings,SorProxSettings2RPlusAndDisk>::TValue SorProxSettings2;
   typedef typename ManualOrDefault<  IsEqual<VariantId,3>::result, TKernelSettings,SorProxSettings1RPlusAndDisk>::TValue SorProxSettings3;
   typedef typename ManualOrDefault<  IsEqual<VariantId,4>::result, TKernelSettings,SorProxSettings1RPlusAndDisk>::TValue SorProxSettings4;

   typedef typename ManualOrDefault<  IsEqual<VariantId,5>::result, TKernelSettings,RelaxedSorProxSettings1RPlusAndDisk >::TValue SorProxSettings5;

   static std::string getVariantName(){
      std::stringstream s;
      switch(VariantId){
      case 1:
         s<< "Full SOR";
         break;
      case 2:
         s<< "Full SOR";
         break;
      case 3:
         s<< "Full SOR";
         break;
      case 4:
         s<< "Full SOR";
         break;
      case 5:
         s<< "Relaxed SOR";
         break;
      default:
         s<< "NO DESCRIPTION: probably wrong VariantId selected!";
         break;
      }
      s << ((bAbortIfConverged)? std::string("[Convergence Check]") : std::string(""));
      return s.str();
   }

   static std::string getVariantDescriptionShort(){
      std::stringstream s;
      switch(VariantId){
      case 1:
         s<< "[Step A][Step B]";
         break;
      case 2:
         s<< "[Step A][Step B]";
         break;
      case 3:
         s<< "[Step A]";
         break;
      case 4:
         s<< "[Step B]";
         break;
      case 5:
         s<< "[Step A][Step B]";
         break;
      default:
         s<< "NO DESCRIPTION: probably wrong VariantId selected!";
         break;
      }
      s << ((bAbortIfConverged)? std::string("[Convergence Check]") : std::string(""));
      return s.str();
   }

   static std::string getVariantDescriptionLong(){
      std::stringstream s;
      switch(VariantId){
      case 1:
         s << SOR_PROXKERNEL_SETTINGS_STR(SorProxSettings1);
         break;
      case 2:
         s << SOR_PROXKERNEL_SETTINGS_STR(SorProxSettings2);
         break;
      case 3:
         s << SOR_PROXKERNEL_A_SETTINGS_STR(SorProxSettings3);
         break;
      case 4:
         s << SOR_PROXKERNEL_B_SETTINGS_STR(SorProxSettings4);
         break;
      case 5:
         s << RELAXED_SOR_PROXKERNEL_SETTINGS_STR(SorProxSettings5);
         break;
      default:
         s << "NO DESCRIPTION: probably wrong VariantId selected!";
         break;
      }
      return s.str() + std::string(", Matrix T aligned: ") + ((alignMatrix)? std::string("on") : std::string("off"));;
   }

   double getNOps(){
      int loops;
      double matrixMultplyFLOPS;
      switch(VariantId){
      case 1:
         return  evaluateProxTermSOR_FLOPS(T_dev.m_M,T_dev.m_N) + proxContactOrdered_RPlusAndDisk_1threads_kernel_FLOPS(T_dev.m_M / SorProxSettings1::ProxPackageSize);
      case 2:
         return  evaluateProxTermSOR_FLOPS(T_dev.m_M,T_dev.m_N) + proxContactOrdered_RPlusAndDisk_1threads_kernel_FLOPS(T_dev.m_M / SorProxSettings2::ProxPackageSize);
      case 3:
         loops = ((T_dev.m_M + ( SorProxSettings3::BlockDimKernelA - 1 ) ) / SorProxSettings3::BlockDimKernelA) ;
         matrixMultplyFLOPS = (loops -1 ) *  (2* SorProxSettings3::BlockDimKernelA *SorProxSettings3::BlockDimKernelA - SorProxSettings3::BlockDimKernelA) ;
         matrixMultplyFLOPS +=    (2*( T_dev.m_M -(loops-1)*SorProxSettings3::BlockDimKernelA)*( T_dev.m_M -(loops-1)*SorProxSettings3::BlockDimKernelA) - ( T_dev.m_M -(loops-1)*SorProxSettings3::BlockDimKernelA));  // So many matrix multiplyses n
         return   matrixMultplyFLOPS + T_dev.m_M + proxContactOrdered_RPlusAndDisk_1threads_kernel_FLOPS(T_dev.m_M / SorProxSettings3::ProxPackageSize);
      case 4:
          loops = ((T_dev.m_M + ( SorProxSettings4::BlockDimKernelA -1 ) ) / SorProxSettings4::BlockDimKernelA) ;
          matrixMultplyFLOPS = 2*T_dev.m_M*T_dev.m_N - T_dev.m_M; // The whole matrix, subtract kernel a below!
           matrixMultplyFLOPS -=    (loops -1 ) *  (2* SorProxSettings4::BlockDimKernelA *SorProxSettings4::BlockDimKernelA - SorProxSettings4::BlockDimKernelA) ;
           matrixMultplyFLOPS -=    (2*( T_dev.m_M -(loops-1)*SorProxSettings4::BlockDimKernelA)*( T_dev.m_M -(loops-1)*SorProxSettings4::BlockDimKernelA) - ( T_dev.m_M -(loops-1)*SorProxSettings4::BlockDimKernelA));  // So many matrix multiplyses n
           return matrixMultplyFLOPS;
      case 5:
           return evaluateProxTermSOR_FLOPS(T_dev.m_M,T_dev.m_N) + proxContactOrdered_RPlusAndDisk_1threads_kernel_FLOPS(T_dev.m_M / SorProxSettings5::ProxPackageSize);
      default:
          return 0;
      }
   }

   double getBytesReadWrite(){
      int loops;
      double matrixMultplyReadWrite;
      double readWriteNumbers;
      switch(VariantId){
      case 1:
         loops = ((T_dev.m_M + ( SorProxSettings1::BlockDimKernelA -1 ) ) / SorProxSettings1::BlockDimKernelA) ;
         readWriteNumbers =  /*T_dev (overall):*/ T_dev.m_M*T_dev.m_N + /*KERNEL A*/ /*t_dev,y_dev :*/ (2*T_dev.m_M+T_dev.m_M) + /*d_dev: */ T_dev.m_M    +   /*KERNEL B*/ /*t_dev:*/ (loops-1) * (T_dev.m_M)   + /*x*/ T_dev.m_M;
         return  readWriteNumbers * sizeof(PREC);
      case 2:
         loops = ((T_dev.m_M + ( SorProxSettings2::BlockDimKernelA -1 ) ) / SorProxSettings2::BlockDimKernelA) ;

         readWriteNumbers =   /*T_dev (overall):*/ T_dev.m_M*T_dev.m_N + /*KERNEL A*/ /*t_dev,y_dev :*/ (2*T_dev.m_M+T_dev.m_M)  + /*d_dev: */ T_dev.m_M    +   /*KERNEL B*/ /*t_dev:*/ (loops-1) * (T_dev.m_M)  + /*x*/ T_dev.m_M;
         return  readWriteNumbers * sizeof(PREC);
      case 3:
            loops = ((T_dev.m_M + ( SorProxSettings3::BlockDimKernelA -1 ) ) / SorProxSettings3::BlockDimKernelA) ;
            matrixMultplyReadWrite = (loops -1 ) *  (SorProxSettings3::BlockDimKernelA *SorProxSettings3::BlockDimKernelA) ;
            matrixMultplyReadWrite +=    (   ( T_dev.m_M -(loops-1)*SorProxSettings3::BlockDimKernelA)*( T_dev.m_M -(loops-1)*SorProxSettings3::BlockDimKernelA)) ;
            readWriteNumbers =    /*KERNEL A*/ matrixMultplyReadWrite + /*t_dev,y_dev :*/ (2*T_dev.m_M+T_dev.m_M) + /*d_dev: */ T_dev.m_M;
            return   readWriteNumbers * sizeof(PREC);
      case 4:
            loops = ((T_dev.m_M + ( SorProxSettings3::BlockDimKernelA -1 ) ) / SorProxSettings3::BlockDimKernelA) ;
            matrixMultplyReadWrite = T_dev.m_M*T_dev.m_N;
            matrixMultplyReadWrite -= (loops -1 ) *  (SorProxSettings4::BlockDimKernelA *SorProxSettings4::BlockDimKernelA) ;
            matrixMultplyReadWrite -=    (( T_dev.m_M -(loops-1)*SorProxSettings4::BlockDimKernelA)*( T_dev.m_M -(loops-1)*SorProxSettings4::BlockDimKernelA));
            readWriteNumbers =   matrixMultplyReadWrite +  /*KERNEL B*/ /*t_dev:*/ (loops-1) * (T_dev.m_M) + /*x*/ T_dev.m_M;
            return   readWriteNumbers * sizeof(PREC);
      case 5:
          loops = ((T_dev.m_M + ( SorProxSettings5::BlockDimKernelA -1 ) ) / SorProxSettings5::BlockDimKernelA) ;
          readWriteNumbers =  /*T_dev (overall):*/ T_dev.m_M*T_dev.m_N + /*KERNEL A*/ /*t_dev,y_dev :*/ (2*T_dev.m_M+T_dev.m_M) + /*d_dev: */ T_dev.m_M    +   /*KERNEL B*/ /*t_dev:*/ (loops) * (T_dev.m_M) + /*x*/ T_dev.m_M;
          return  readWriteNumbers * sizeof(PREC);
      default:
         return 0;
      }
   }


   unsigned int getTradeoff(){
     switch(VariantId){
      case 1:
         return 400;
      case 2:
         return 400;
      case 5:
         if(SorProxSettings5::ProxPackages == 2){
            return 550;
         }
         else if(SorProxSettings5::ProxPackages == 4){
            return 450;
         }
         else if(SorProxSettings5::ProxPackages == 8){
            return 350;
         }
         else if(SorProxSettings5::ProxPackages == 16){
            return 300;
         }
         else if(SorProxSettings5::ProxPackages == 32){
            return 260;
         }
         else if(SorProxSettings5::ProxPackages == 64 || SorProxSettings5::ProxPackages == 128){
            return 200;
         }else{
            return 300;
         }
      default:
         return 200;
      }
   }


   void initialize(std::ostream* pLog, std::ofstream* pMatlab_file){
      m_pLog = pLog;
      m_pMatlab_file = pMatlab_file;
   }

   void finalize(){

   }

   void setDeviceToUse(int device){
      CHECK_CUDA(cudaSetDevice(device));
   }

   template<typename Derived1, typename Derived2>
   void initializeTestProblem( const Eigen::MatrixBase<Derived2> &T, const Eigen::MatrixBase<Derived1> & x_old,const Eigen::MatrixBase<Derived1> & d){

      CHECK_CUBLAS(cublasCreate(&m_cublasHandle));
      CHECK_CUDA(cudaEventCreate(&m_startKernel));
      CHECK_CUDA(cudaEventCreate(&m_stopKernel));
      CHECK_CUDA(cudaEventCreate(&m_startCopy));
      CHECK_CUDA(cudaEventCreate(&m_stopCopy));

      size_t freeMem;
      size_t totalMem;
      CHECK_CUDA(cudaMemGetInfo (&freeMem, &totalMem));

     size_t nGPUBytes = (3*x_old.rows() + d.rows() + x_old.rows()/TConvexSet::Dimension +  T.rows()*T.cols())*sizeof(PREC);
      *m_pLog << "Will try to allocate ("<<nGPUBytes<<"/"<<freeMem<<") = " << (double)nGPUBytes/freeMem * 100.0 <<" % of global memory on GPU, Total mem: "<<totalMem<<std::endl;
      if(nGPUBytes > freeMem){
         *m_pLog <<"Probably to little memory on GPU, try anway!..."<<std::endl;
      }

      cudaError_t error;

     CHECK_CUDA(utilCuda::releaseAndMallocMatrixDevice<alignMatrix>(T_dev, T));
     CHECK_CUDA(utilCuda::releaseAndMallocMatrixDevice<false>(d_dev, d));
     CHECK_CUDA(utilCuda::releaseAndMallocMatrixDevice<false>(x_old_dev,x_old));
     CHECK_CUDA(utilCuda::releaseAndMallocMatrixDevice<false>(t_dev,x_old));

      if(TypeTraitsHelper::IsSame<TConvexSet,ConvexSets::RPlusAndDisk>::result){
         ASSERTMSG(x_old.rows()%TConvexSet::Dimension==0,"Wrong Dimension!");
         CHECK_CUDA(utilCuda::releaseAndMallocMatrixDevice<false>(mu_dev, x_old.rows()/TConvexSet::Dimension, x_old.cols()));
      }
      else if(TypeTraitsHelper::IsSame<TConvexSet,ConvexSets::RPlusAndContensouEllipsoid>::result){
         ASSERTMSG(false, "NOT IMPLEMENTED!");
      }

     CHECK_CUDA((utilCuda::releaseAndMallocMatrixDevice<false>(x_new_dev,x_old_dev.m_M,x_old_dev.m_N)));

     if(pConvergedFlag_dev){
         CHECK_CUDA(cudaFree(pConvergedFlag_dev));
         pConvergedFlag_dev = NULL;
      }
     CHECK_CUDA(cudaMalloc(&pConvergedFlag_dev,sizeof(bool)));

   }


   template<typename Derived1, typename Derived2,typename Derived3>
   void runCPUEquivalentProfile(Eigen::MatrixBase<Derived1> & x_newCPU, const Eigen::MatrixBase<Derived2> &T, Eigen::MatrixBase<Derived1> & x_old, const Eigen::MatrixBase<Derived1> & d, const Eigen::MatrixBase<Derived3> & mu){

      // Do Sor Scheme!
         static const int ProxPackageSize = TConvexSet::Dimension;

         Derived1  x_temp;
          bool m_bConverged = false;

         // Do SOR Scheme!
         START_TIMER(start)

               if(VariantId < 5 ){

                  unsigned int loops =  T.cols() / ProxPackageSize;

                  if(bMatchCPUToGPU){
                  // IF we match the GPU we implement the identical algorithm on the CPU!
                    for(m_nIterCPU=0; m_nIterCPU< m_nMaxIterations ; m_nIterCPU++)
                     {

                         if(bAbortIfConverged){
                           x_temp.noalias() = x_old;
                        }

                        for(unsigned int i=0; i < loops; i++){
                           // Normal
                           x_old((ProxPackageSize)*i) = (T.row((ProxPackageSize)*i) * x_old) + d((ProxPackageSize)*i);
                           Prox::ProxFunction<ConvexSets::RPlus>::doProxSingle(x_old((ProxPackageSize)*i));
                           // Tangential
                           x_old.template segment<ProxPackageSize-1 >((ProxPackageSize)*i+1)  = (T.block((ProxPackageSize)*i+1,0,ProxPackageSize-1,T.cols()) * x_old) + d.template segment<ProxPackageSize-1 >((ProxPackageSize)*i+1) ;
                           Prox::ProxFunction<ConvexSets::Disk>::doProxSingle(mu(i) *  x_old((ProxPackageSize)*i) , x_old.template segment<ProxPackageSize-1 >((ProxPackageSize)*i+1) );
                        }

                        if(bAbortIfConverged){
                              // Check each nCheckConvergedFlag  the converged flag
                              if(m_nIterCPU % nCheckConvergedFlag == 0){
                                    //Calculate CancelCriteria
                                    m_bConverged = Numerics::cancelCriteriaValue(x_old,x_temp, m_absTOL, m_relTOL);
                                    if (m_bConverged == true)
                                    {
                                       break;
                                    }
                              }
                        }

                     }
                  }else{
                    for(m_nIterCPU=0; m_nIterCPU< m_nMaxIterations ; m_nIterCPU++)
                     {

                        if(bAbortIfConverged){
                           x_temp.noalias() = x_old;
                        }

                        for(unsigned int i=0; i < loops; i++){
                           // Normal and Tangential together ( is not the same as the SOR scheme on the GPU, but they converge to the same!)
                           x_old.template segment<ProxPackageSize>((ProxPackageSize)*i)  = (T.block((ProxPackageSize)*i,0,ProxPackageSize,T.cols()) * x_old) + d.template segment<ProxPackageSize>((ProxPackageSize)*i) ;
                           Prox::ProxFunction<ConvexSets::RPlusAndDisk>::doProxSingle(mu(i), x_old.template segment<ProxPackageSize>((ProxPackageSize)*i));
                        }

                        if(bAbortIfConverged){
                              // Check each nCheckConvergedFlag  the converged flag
                              if(m_nIterCPU % nCheckConvergedFlag == 0){
                                    //Calculate CancelCriteria
                                    m_bConverged = Numerics::cancelCriteriaValue(x_old,x_temp, m_absTOL, m_relTOL);
                                    if (m_bConverged == true)
                                    {
                                       break;
                                    }
                              }
                        }

                     }
                  }

               }
               else if(VariantId == 5){
                  const int ProxPackages = SorProxSettings5::ProxPackages;
                  const int BlockDimKernelA = SorProxSettings5::BlockDimKernelA;

                  unsigned int loops =  (T.rows() + (SorProxSettings5::BlockDimKernelA-1)) / (SorProxSettings5::BlockDimKernelA);
                  unsigned int i=0;
                  unsigned int restSize,restContacts;

                  for(m_nIterCPU=0; m_nIterCPU< m_nMaxIterations ; m_nIterCPU++){

                        if(bAbortIfConverged){
                           x_temp.noalias() = x_old;
                        }


                        for(i = 0; i < loops-1; i++){
                           // IF we match or not match the GPU, we implement the identical algorithm on the CPU!
                              x_old.template segment<BlockDimKernelA>((BlockDimKernelA)*i)  = (T.block((BlockDimKernelA)*i,0,BlockDimKernelA,T.cols()) * x_old) + d.template segment<BlockDimKernelA>((BlockDimKernelA)*i) ;
                              Prox::ProxFunction<ConvexSets::RPlusAndDisk>::doProxMulti(mu.template segment<ProxPackages>(ProxPackages*i), x_old.template segment<BlockDimKernelA>((BlockDimKernelA)*i));
                        }
                        // Do the rest
                        restSize = T.cols() - (loops-1)*BlockDimKernelA;
                        restContacts = restSize / ProxPackageSize;
                        if(restSize !=0){
                           x_old.segment((BlockDimKernelA)*i, restSize)  = (T.block((BlockDimKernelA)*i,0,restSize,T.cols()) * x_old) + d.segment((BlockDimKernelA)*i,restSize) ;
                           Prox::ProxFunction<ConvexSets::RPlusAndDisk>::doProxMulti(mu.segment(ProxPackages*i,restContacts), x_old.segment((BlockDimKernelA)*i,restSize));
                        }

                        if(bAbortIfConverged){
                              // Check each nCheckConvergedFlag  the converged flag
                              if(m_nIterCPU % nCheckConvergedFlag == 0){
                                    //Calculate CancelCriteria
                                    m_bConverged = Numerics::cancelCriteriaValue(x_old,x_temp, m_absTOL, m_relTOL);
                                    if (m_bConverged == true)
                                    {
                                       break;
                                    }
                              }
                        }

                  }
           }

         // Do a swap!
         x_old.swap(x_newCPU.derived());

      STOP_TIMER_NANOSEC(count,start)

      m_cpuIterationTime = (count*1e-6 / m_nIterCPU);

      *m_pLog << " ---> CPU Sequential Iteration time: " <<  boost::format("%1$8.6f ms") % (m_cpuIterationTime) <<std::endl;
      *m_pLog << " ---> nIterations: " << m_nIterCPU <<std::endl;
      if (m_nIterCPU == nMaxIterations){
         *m_pLog << " ---> Not converged! Max. Iterations reached."<<std::endl;
      }
   }

   template<typename Derived1, typename Derived2,typename Derived3>
   void runCPUEquivalentPlain(Eigen::MatrixBase<Derived1> & x_newCPU, const Eigen::MatrixBase<Derived2> &T, Eigen::MatrixBase<Derived1> & x_old, const Eigen::MatrixBase<Derived1> & d, const Eigen::MatrixBase<Derived3> & mu){

      // Do Sor Scheme!
         static const int ProxPackageSize = TConvexSet::Dimension;
         // Prox the contact

         //cout << typeid(Derived1).name() << endl << typeid(Derived1::PlainObject).name() <<endl;
         static typename Derived1::PlainObject  x_temp;

          // Do SOR Scheme!
         START_TIMER(start)

         bool m_bConverged = false;

               if(VariantId < 5 ){

                  unsigned int loops =  T.cols() / ProxPackageSize;

                  if(bMatchCPUToGPU){
                  // IF we match the GPU we implement the identical algorithm on the CPU!
                    for(m_nIterCPU=0; m_nIterCPU< m_nMaxIterations ; m_nIterCPU++)
                     {

                         if(bAbortIfConverged){
                           x_temp.noalias() = x_old;
                        }

                        for(unsigned int i=0; i < loops; i++){
                           // Normal
                           x_old((ProxPackageSize)*i) = (T.row((ProxPackageSize)*i) * x_old) + d((ProxPackageSize)*i);
                           Prox::ProxFunction<ConvexSets::RPlus>::doProxSingle(x_old((ProxPackageSize)*i));
                           // Tangential
                           x_old.template segment<ProxPackageSize-1>((ProxPackageSize)*i+1)  = (T.template block((ProxPackageSize)*i+1,0,ProxPackageSize-1,T.cols()) * x_old) + d.template segment<ProxPackageSize-1>((ProxPackageSize)*i+1) ;
                           Prox::ProxFunction<ConvexSets::Disk>::doProxSingle(mu(i) *  x_old((ProxPackageSize)*i) , x_old.template segment<ProxPackageSize-1>((ProxPackageSize)*i+1) );
                        }

                        if(bAbortIfConverged){
                              // Check each nCheckConvergedFlag  the converged flag
                              if(m_nIterCPU % nCheckConvergedFlag == 0){
                                    //Calculate CancelCriteria
                                    m_bConverged = Numerics::cancelCriteriaValue(x_old,x_temp, m_absTOL, m_relTOL);
                                    if (m_bConverged == true)
                                    {
                                       break;
                                    }
                              }
                        }

                     }
                  }else{
                    for(m_nIterCPU=0; m_nIterCPU< m_nMaxIterations ; m_nIterCPU++)
                     {

                        if(bAbortIfConverged){
                           x_temp.noalias() = x_old;
                        }

                        for(unsigned int i=0; i < loops; i++){
                           // Normal and Tangential together ( is not the same as the SOR scheme on the GPU, but the converge to the same!)
                           x_old.template segment<ProxPackageSize>((ProxPackageSize)*i)  = (T.template block((ProxPackageSize)*i,0,ProxPackageSize,T.cols()) * x_old) + d.template segment<ProxPackageSize>((ProxPackageSize)*i) ;
                           Prox::ProxFunction<ConvexSets::RPlusAndDisk>::doProxSingle(mu(i), x_old.template segment<ProxPackageSize>((ProxPackageSize)*i));
                        }

                        if(bAbortIfConverged){
                              // Check each nCheckConvergedFlag  the converged flag
                              if(m_nIterCPU % nCheckConvergedFlag == 0){
                                    //Calculate CancelCriteria
                                    m_bConverged = Numerics::cancelCriteriaValue(x_old,x_temp, m_absTOL, m_relTOL);
                                    if (m_bConverged == true)
                                    {
                                       break;
                                    }
                              }
                        }

                     }
                  }

               }
               else if(VariantId == 5){
                  const int ProxPackages = SorProxSettings5::ProxPackages;
                  const int BlockDimKernelA = SorProxSettings5::BlockDimKernelA;

                  unsigned int loops =  (T.rows() + (SorProxSettings5::BlockDimKernelA-1)) / (SorProxSettings5::BlockDimKernelA);
                  unsigned int i=0;
                  unsigned int restSize,restContacts;

                  for(m_nIterCPU=0; m_nIterCPU< m_nMaxIterations ; m_nIterCPU++){

                        if(bAbortIfConverged){
                           x_temp.noalias() = x_old;
                        }


                        for(i = 0; i < loops-1; i++){
                           // IF we match or not match the GPU, we implement the identical algorithm on the CPU!
                              x_old.template segment<BlockDimKernelA>((BlockDimKernelA)*i)  = (T.template block((BlockDimKernelA)*i,0,BlockDimKernelA,T.cols()) * x_old) + d.template segment<BlockDimKernelA>((BlockDimKernelA)*i) ;
                              Prox::ProxFunction<ConvexSets::RPlusAndDisk>::doProxMulti(mu.template segment<ProxPackages>(ProxPackages*i), x_old.template segment<BlockDimKernelA>((BlockDimKernelA)*i));
                        }
                        // Do the rest
                        restSize = T.cols() - (loops-1)*BlockDimKernelA;
                        restContacts = restSize / ProxPackageSize;
                        if(restSize !=0){
                           x_old.segment((BlockDimKernelA)*i, restSize)  = (T.block((BlockDimKernelA)*i,0,restSize,T.cols()) * x_old) + d.segment((BlockDimKernelA)*i,restSize) ;
                           Prox::ProxFunction<ConvexSets::RPlusAndDisk>::doProxMulti(mu.segment(ProxPackages*i,restContacts), x_old.segment((BlockDimKernelA)*i,restSize));
                        }

                        if(bAbortIfConverged){
                              // Check each nCheckConvergedFlag  the converged flag
                              if(m_nIterCPU % nCheckConvergedFlag == 0){
                                    //Calculate CancelCriteria
                                    m_bConverged = Numerics::cancelCriteriaValue(x_old,x_temp, m_absTOL, m_relTOL);
                                    if (m_bConverged == true)
                                    {
                                       break;
                                    }
                              }
                        }

                  }
           }else if(VariantId == 11){
               //std::cout<< "SOR VARIANT 11"<<std::endl;
                // This is a SOR Special where we iterate several time over one contact!
                // Just for test, has no implementation in GPU!!
                  // IF we match the GPU we implement the identical algorithm on the CPU!
                    unsigned int loops =  T.cols() / ProxPackageSize;

                    for(m_nIterCPU=0; m_nIterCPU< m_nMaxIterations ; m_nIterCPU++)
                     {

                         if(bAbortIfConverged){
                           x_temp.noalias() = x_old;
                        }

                        for(unsigned int i=0; i < loops; i++){
                           // Internal loop!
                           for(unsigned int intLoop=0; intLoop < 5; intLoop++){
                               // Normal
                               x_old((ProxPackageSize)*i) = (T.row((ProxPackageSize)*i) * x_old) + d((ProxPackageSize)*i);
                               Prox::ProxFunction<ConvexSets::RPlus>::doProxSingle(x_old((ProxPackageSize)*i));
                               // Tangential
                               x_old.template segment<ProxPackageSize-1>((ProxPackageSize)*i+1)  = (T.template block((ProxPackageSize)*i+1,0,ProxPackageSize-1,T.cols()) * x_old) + d.template segment<ProxPackageSize-1>((ProxPackageSize)*i+1) ;
                               Prox::ProxFunction<ConvexSets::Disk>::doProxSingle(mu(i) *  x_old((ProxPackageSize)*i) , x_old.template segment<ProxPackageSize-1>((ProxPackageSize)*i+1) );
                           }
                        }

                        if(bAbortIfConverged){
                              // Check each nCheckConvergedFlag  the converged flag
                              if(m_nIterCPU % nCheckConvergedFlag == 0){
                                    //Calculate CancelCriteria
                                    m_bConverged = Numerics::cancelCriteriaValue(x_old,x_temp, m_absTOL, m_relTOL);
                                    if (m_bConverged == true)
                                    {
                                       break;
                                    }
                              }
                        }

                     }
           }

         // Do a swap!
         x_old.swap(x_newCPU.derived());

         STOP_TIMER_NANOSEC(count,start)
         m_cpuIterationTime = (count*1e-6 / m_nIterCPU);
   }


   template<typename Derived1, typename Derived2,typename Derived3>
   void runGPUProfile(Eigen::MatrixBase<Derived1> & x_newGPU, const Eigen::MatrixBase<Derived2> &T, const Eigen::MatrixBase<Derived1> & x_old, const Eigen::MatrixBase<Derived1> & d, const Eigen::MatrixBase<Derived3> & mu){

      ASSERTMSG(x_newGPU.rows() == x_old.rows(), "Wrong Dimensions");
      ASSERTMSG(T.cols() == T.rows(), "Wrong Dimensions");
      ASSERTMSG(x_old.rows() == d.rows(), "Wrong Dimensions");
      ASSERTMSG(mu.rows() * TConvexSet::Dimension == x_old.rows(), mu.rows() * TConvexSet::Dimension << " , " << x_old.rows() << "Wrong Dimensions" );



      // Calulate t_dev and upload!
      m_t.resize(x_old.rows());
      m_t.setZero();


      //Copy Data
      CHECK_CUDA(cudaEventRecord(m_startCopy,0));
      CHECK_CUDA(copyMatrixToDevice(T_dev, T));
      CHECK_CUDA(copyMatrixToDevice(d_dev, d));
      CHECK_CUDA(copyMatrixToDevice(x_old_dev,x_old));
      CHECK_CUDA(copyMatrixToDevice(mu_dev,mu));
      CHECK_CUDA(copyMatrixToDevice(t_dev,m_t));
      CHECK_CUDA(cudaEventRecord(m_stopCopy,0));
      CHECK_CUDA(cudaEventSynchronize(m_stopCopy));



      float time;
      CHECK_CUDA( cudaEventElapsedTime(&time,m_startCopy,m_stopCopy));
      m_elapsedTimeCopyToGPU = time;
      *m_pLog << " ---> Copy time to GPU:"<< boost::format("%1$8.6f ms") % time <<std::endl;


      *m_pLog << " ---> Iterations started..."<<std::endl;

      m_absTOL = 1e-8;
      m_relTOL = 1e-10;

      CHECK_CUDA(cudaEventRecord(m_startKernel,0));

      runKernel();

      CHECK_CUDA(cudaEventRecord(m_stopKernel,0));
      CHECK_CUDA(cudaThreadSynchronize());

      CHECK_CUDA(cudaEventSynchronize(m_stopKernel));
      CHECK_CUDA(cudaGetLastError());


      *m_pLog<<" ---> Iterations finished" << std::endl;
      CHECK_CUDA( cudaEventElapsedTime(&time,m_startKernel,m_stopKernel));
      double average = (time/(double)m_nIterGPU);
      m_gpuIterationTime = average;

      *m_pLog << " ---> GPU Iteration time :"<< boost::format("%1$8.6f ms") % (average) <<std::endl;
      *m_pLog << " ---> nIterations: " << m_nIterGPU <<std::endl;
      if (m_nIterGPU == nMaxIterations){
         *m_pLog << " ---> Max. Iterations reached."<<std::endl;
      }

      //Sleep(500);

      // Copy results back
      CHECK_CUDA(cudaEventRecord(m_startCopy,0));
      CHECK_CUDA(copyMatrixToHost(x_newGPU,x_new_dev));
      CHECK_CUDA(cudaEventRecord(m_stopCopy,0));
      CHECK_CUDA(cudaEventSynchronize(m_stopCopy));
      CHECK_CUDA( cudaEventElapsedTime(&time,m_startCopy,m_stopCopy));
      m_elapsedTimeCopyFromGPU = time;
      *m_pLog << " ---> Copy time from GPU:"<< boost::format("%1$8.6f ms") % time <<std::endl;
   }

   template<typename Derived1, typename Derived2, typename Derived3>
   bool runGPUPlain(Eigen::MatrixBase<Derived1> & x_newGPU, const Eigen::MatrixBase<Derived2> &T, const Eigen::MatrixBase<Derived1> & x_old, const Eigen::MatrixBase<Derived1> & d, const Eigen::MatrixBase<Derived3> & mu){

      ASSERTMSG(x_newGPU.rows() == x_old.rows(), "Wrong Dimensions");
      ASSERTMSG(T.cols() == T.rows(), "Wrong Dimensions");
      ASSERTMSG(x_old.rows() == d.rows(), "Wrong Dimensions");
      ASSERTMSG(mu.rows() * TConvexSet::Dimension == x_old.rows(), mu.rows() * TConvexSet::Dimension << " , " << x_old.rows() << "Wrong Dimensions" );

      cudaError_t error;

      error = utilCuda::releaseAndMallocMatrixDevice<alignMatrix>(T_dev, T); CHECK_CUDA(error);
      error = utilCuda::releaseAndMallocMatrixDevice<false>(d_dev, d); CHECK_CUDA(error);
      error = utilCuda::releaseAndMallocMatrixDevice<false>(x_old_dev,x_old); CHECK_CUDA(error);
      error = utilCuda::releaseAndMallocMatrixDevice<false>(t_dev,x_old); CHECK_CUDA(error);

      if(TypeTraitsHelper::IsSame<TConvexSet,ConvexSets::RPlusAndDisk>::result){
         ASSERTMSG(x_old.rows()%TConvexSet::Dimension==0,"Wrong Dimension!");
         error = utilCuda::releaseAndMallocMatrixDevice<false>(mu_dev, x_old.rows()/TConvexSet::Dimension, x_old.cols()); CHECK_CUDA(error);
      }
      else if(TypeTraitsHelper::IsSame<TConvexSet,ConvexSets::RPlusAndContensouEllipsoid>::result){
         ASSERTMSG(false, "NOT IMPLEMENTED!");
      }

      error = utilCuda::releaseAndMallocMatrixDevice<false>(x_new_dev,x_old_dev.m_M,x_old_dev.m_N); CHECK_CUDA(error);
      error = cudaMalloc(&pConvergedFlag_dev,sizeof(bool)); CHECK_CUDA(error);


      // Calculate t_dev and upload!
      m_t.resize(x_old.rows());
      m_t.setZero();


      //Copy Data
      copyMatrixToDevice(T_dev, T);
      copyMatrixToDevice(d_dev, d);
      copyMatrixToDevice(x_old_dev,x_old);
      copyMatrixToDevice(mu_dev,mu);
      copyMatrixToDevice(t_dev,m_t);

         cudaEventCreate(&m_startKernel);
         cudaEventCreate(&m_stopKernel);

         cudaEventRecord(m_startKernel,0);
         runKernel();
         cudaEventRecord(m_stopKernel,0);


         cudaThreadSynchronize();

         float time;
         cudaEventElapsedTime(&time,m_startKernel,m_stopKernel);
         m_gpuIterationTime = (time/(double)m_nIterGPU);


      copyMatrixToHost(x_newGPU,x_new_dev);

      cudaEventDestroy(m_startKernel);
      cudaEventDestroy(m_stopKernel);

      return true;
   }

   // Partial Specialization of Member Function in template Class is not allowed.. Can do alternative struct wrapping, but this gets nasty!
   void runKernel(){

      if(VariantId == 1){

         // Swap pointers of old and new on the device
         std::swap(x_old_dev.m_pDevice, x_new_dev.m_pDevice);

         int loops = (T_dev.m_M + ( SorProxSettings1::BlockDimKernelA -1 ) ) / SorProxSettings1::BlockDimKernelA ;

        for(m_nIterGPU=0; m_nIterGPU< m_nMaxIterations ; m_nIterGPU++){

            // Swap pointers of old and new on the device
            std::swap(x_old_dev.m_pDevice, x_new_dev.m_pDevice);

            for(int kernelAIdx = 0; kernelAIdx < loops; kernelAIdx++){

               //cudaThreadSynchronize();
               proxGPU::sorProxContactOrdered_1threads_StepA_kernelWrap<SorProxSettings1>(
                  mu_dev,x_new_dev,T_dev,d_dev,
                  t_dev,
                  kernelAIdx,
                  pConvergedFlag_dev,
                  m_absTOL,m_relTOL);

               proxGPU::sorProx_StepB_kernelWrap<SorProxSettings1>(
                  t_dev,
                  T_dev,
                  x_new_dev,
                  kernelAIdx
                  );

            }


            if(bAbortIfConverged){
               // Check each nCheckConvergedFlag  the converged flag
               if(m_nIterGPU % nCheckConvergedFlag == 0){
                  // Check convergence
                   // First set the converged flag to 1
                  cudaMemset(pConvergedFlag_dev,1,sizeof(bool));
                  proxGPU::convergedEach_kernelWrap(x_new_dev,x_old_dev,pConvergedFlag_dev,m_absTOL,m_relTOL);

                  // Download the flag from the GPU and check
                  cudaMemcpy(&m_bConvergedFlag,pConvergedFlag_dev,sizeof(bool),cudaMemcpyDeviceToHost);
                  if(m_bConvergedFlag == true){
                     // Converged
                     break;
                  }
               }
            }


         }
      }
      else if(VariantId == 2){

         // Swap pointers of old and new on the device
         std::swap(x_old_dev.m_pDevice, x_new_dev.m_pDevice);

         int loops = (T_dev.m_M + ( SorProxSettings2::BlockDimKernelA -1 ) ) / SorProxSettings2::BlockDimKernelA ;

         for(m_nIterGPU=0; m_nIterGPU< m_nMaxIterations ; m_nIterGPU++){

            // Swap pointers of old and new on the device
            std::swap(x_old_dev.m_pDevice, x_new_dev.m_pDevice);

            for(int kernelAIdx = 0; kernelAIdx < loops; kernelAIdx++){

               proxGPU::sorProxContactOrdered_1threads_StepA_kernelWrap<SorProxSettings2>(
                  mu_dev,x_new_dev,T_dev,d_dev,
                  t_dev,
                  kernelAIdx,
                  pConvergedFlag_dev,
                  m_absTOL,m_relTOL);

               proxGPU::sorProx_StepB_kernelWrap<SorProxSettings2>(
                  t_dev,
                  T_dev,
                  x_new_dev,
                  kernelAIdx
                  );
            }


            if(bAbortIfConverged){
               // Check each nCheckConvergedFlag  the converged flag
               if(m_nIterGPU % nCheckConvergedFlag == 0){
                  // First set the converged flag to 1
                  cudaMemset(pConvergedFlag_dev,1,sizeof(bool));
                  // Check convergence
                  proxGPU::convergedEach_kernelWrap(x_new_dev,x_old_dev,pConvergedFlag_dev,m_absTOL,m_relTOL);

                  // Download the flag from the GPU and check
                  cudaMemcpy(&m_bConvergedFlag,pConvergedFlag_dev,sizeof(bool),cudaMemcpyDeviceToHost);
                  if(m_bConvergedFlag == true){
                     // Converged
                     break;
                  }
               }
            }
         }
      }
      else if(VariantId == 3){

         // Swap pointers of old and new on the device
         std::swap(x_old_dev.m_pDevice, x_new_dev.m_pDevice);

         int loops = (T_dev.m_M + ( SorProxSettings3::BlockDimKernelA -1 ) ) / SorProxSettings3::BlockDimKernelA ;
         for(m_nIterGPU=0; m_nIterGPU< m_nMaxIterations ; m_nIterGPU++){

            // Swap pointers of old and new on the device
            std::swap(x_old_dev.m_pDevice, x_new_dev.m_pDevice);

            for(int kernelAIdx = 0; kernelAIdx < loops; kernelAIdx++){

               proxGPU::sorProxContactOrdered_1threads_StepA_kernelWrap<SorProxSettings3>(
                  mu_dev,x_new_dev,T_dev,d_dev,
                  t_dev,
                  kernelAIdx,
                  pConvergedFlag_dev,
                  m_absTOL,m_relTOL);

             /*  proxGPU::sorProx_StepB_kernelWrap<SorProxSettings3>(
                  t_dev,
                  T_dev,
                  x_new_dev,
                  kernelAIdx
                  );*/
            }
         }
      }
      else if(VariantId == 4){

          // Swap pointers of old and new on the device
         std::swap(x_old_dev.m_pDevice, x_new_dev.m_pDevice);

         int loops = (T_dev.m_M + ( SorProxSettings4::BlockDimKernelA -1 ) ) / SorProxSettings4::BlockDimKernelA ;

         for(m_nIterGPU=0; m_nIterGPU< m_nMaxIterations ; m_nIterGPU++){

            // Swap pointers of old and new on the device
            std::swap(x_old_dev.m_pDevice, x_new_dev.m_pDevice);

            for(int kernelAIdx = 0; kernelAIdx < loops; kernelAIdx++){

              /* proxGPU::sorProxContactOrdered_1threads_StepA_kernelWrap<SorProxSettings4>(
                  mu_dev,x_new_dev,T_dev,x_old_dev,d_dev,
                  t_dev,
                  kernelAIdx,
                  pConvergedFlag_dev,
                  m_absTOL,m_relTOL);*/

               proxGPU::sorProx_StepB_kernelWrap<SorProxSettings4>(
                  t_dev,
                  T_dev,
                  x_new_dev,
                  kernelAIdx
                  );
            }
         }
      }
      else if(VariantId == 5){

         // Swap pointers of old and new on the device
         std::swap(x_old_dev.m_pDevice, x_new_dev.m_pDevice);

         int loops = (T_dev.m_M + ( SorProxSettings5::BlockDimKernelA -1 ) ) / SorProxSettings5::BlockDimKernelA ;

         for(m_nIterGPU=0; m_nIterGPU< m_nMaxIterations ; m_nIterGPU++){

            // Swap pointers of old and new on the device
            std::swap(x_old_dev.m_pDevice, x_new_dev.m_pDevice);

            for(int kernelAIdx = 0; kernelAIdx < loops; kernelAIdx++){

               proxGPU::sorProxRelaxed_StepA_kernelWrap<SorProxSettings5>(
                  mu_dev,
                  x_new_dev,
                  t_dev,
                  d_dev,
                  kernelAIdx
               );

               proxGPU::sorProxRelaxed_StepB_kernelWrap<SorProxSettings5>(
                  t_dev,
                  T_dev,
                  x_new_dev,
                  kernelAIdx
                  );
            }


            if(bAbortIfConverged){
               // Check each nCheckConvergedFlag  the converged flag
               if(m_nIterGPU % nCheckConvergedFlag == 0){
                  // First set the converged flag to 1
                  cudaMemset(pConvergedFlag_dev,1,sizeof(bool));

                  // Check convergence
                  proxGPU::convergedEach_kernelWrap(x_new_dev,x_old_dev,pConvergedFlag_dev,m_absTOL,m_relTOL);

                  // Download the flag from the GPU and check
                  cudaMemcpy(&m_bConvergedFlag,pConvergedFlag_dev,sizeof(bool),cudaMemcpyDeviceToHost);
                  if(m_bConvergedFlag == true){
                     // Converged
                     break;
                  }
               }
            }
         }
      }

   }



   void cleanUpTestProblem(){

      CHECK_CUDA(freeMatrixDevice(T_dev));
      CHECK_CUDA(freeMatrixDevice(d_dev));
      CHECK_CUDA(freeMatrixDevice(x_old_dev));
      CHECK_CUDA(freeMatrixDevice(x_new_dev));
      CHECK_CUDA(freeMatrixDevice(t_dev));
      CHECK_CUDA(freeMatrixDevice(mu_dev));
      if(pConvergedFlag_dev){
         CHECK_CUDA(cudaFree(pConvergedFlag_dev));
         pConvergedFlag_dev = NULL;
      }

      CHECK_CUDA(cudaEventDestroy(m_startKernel));
      CHECK_CUDA(cudaEventDestroy(m_stopKernel));
      CHECK_CUDA(cudaEventDestroy(m_startCopy));
      CHECK_CUDA(cudaEventDestroy(m_stopCopy));

   }

   double m_gpuIterationTime;
   double m_elapsedTimeCopyToGPU;
   double m_elapsedTimeCopyFromGPU;
   int m_nIterGPU;

   double m_cpuIterationTime;
   int m_nIterCPU;

private:
   Eigen::Matrix<PREC,Eigen::Dynamic,1> m_t;
   utilCuda::CudaMatrix<PREC> T_dev, d_dev, x_old_dev, x_new_dev, mu_dev, t_dev;
   bool *pConvergedFlag_dev;
   bool m_bConvergedFlag;

   unsigned int m_nMaxIterations;
   PREC m_absTOL, m_relTOL;

   cublasHandle_t m_cublasHandle;
   cudaEvent_t m_startKernel, m_stopKernel, m_startCopy,m_stopCopy;

   std::ostream* m_pLog;
   std::ofstream* m_pMatlab_file;
   Eigen::IOFormat Matlab;
};




/** @} */


#endif
